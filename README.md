# multi-model-inference-accl
use TorchServe and Triton to accelerate multiple model serving 
